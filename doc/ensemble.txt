Version 1.0.1

Check your versions
In [1]:

import numpy as np
import pandas as pd 
import sklearn
import scipy.sparse 
import lightgbm 
​
for p in [np, pd, scipy, sklearn, lightgbm]:
    print (p.__name__, p.__version__)
numpy 1.13.1
pandas 0.20.3
scipy 0.19.1
sklearn 0.19.0
lightgbm 2.0.6
Important! There is a huge chance that the assignment will be impossible to pass if the versions of lighgbm and scikit-learn are wrong. The versions being tested:

numpy 1.13.1
pandas 0.20.3
scipy 0.19.1
sklearn 0.19.0
ligthgbm 2.0.6
To install an older version of lighgbm you may use the following command:

pip uninstall lightgbm
pip install lightgbm==2.0.6
Ensembling
In this programming assignment you are asked to implement two ensembling schemes: simple linear mix and stacking.

We will spend several cells to load data and create feature matrix, you can scroll down this part or try to understand what's happening.

In [2]:

LinearRegression
import pandas as pd
import numpy as np
import gc
import matplotlib.pyplot as plt
%matplotlib inline 
​
pd.set_option('display.max_rows', 600)
pd.set_option('display.max_columns', 50)
​
import lightgbm as lgb
from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
from tqdm import tqdm_notebook
​
from itertools import product
​
​
def downcast_dtypes(df):
    '''
        Changes column types in the dataframe: 
                
                `float64` type to `float32`
                `int64`   type to `int32`
    '''
    
    # Select columns to downcast
    float_cols = [c for c in df if df[c].dtype == "float64"]
    int_cols =   [c for c in df if df[c].dtype == "int64"]
    
    # Downcast
    df[float_cols] = df[float_cols].astype(np.float32)
    df[int_cols]   = df[int_cols].astype(np.int32)
    
    return df
Load data subset
Let's load the data from the hard drive first.

In [3]:

sales
sales = pd.read_csv('../readonly/final_project_data/sales_train.csv.gz')
shops = pd.read_csv('../readonly/final_project_data/shops.csv')
items = pd.read_csv('../readonly/final_project_data/items.csv')
item_cats = pd.read_csv('../readonly/final_project_data/item_categories.csv')
And use only 3 shops for simplicity.

In [8]:

sales
sales = sales[sales['shop_id'].isin([26, 27, 28])]
In [23]:

sales.date_block_num.value_counts().sort_index()
Out[23]:
0     12560
1     10358
2     12086
3      9512
4      9144
5      7464
6     10724
7     11805
8     10045
9      9127
10     9807
11    14825
12    11352
13     9213
14     9657
15     7594
16     8637
17     9024
18     8151
19     9339
20     7104
21     7953
22     8806
23    13193
24    10735
25     7828
26     7949
27     6474
28     7032
29     6908
30     6050
31     4155
32     3265
33     3634
Name: date_block_num, dtype: int64
Get a feature matrix
We now need to prepare the features. This part is all implemented for you.

In [9]:

date_block_num
# Create "grid" with columns
index_cols = ['shop_id', 'item_id', 'date_block_num']
​
# For every month we create a grid from all shops/items combinations from that month
grid = [] 
for block_num in sales['date_block_num'].unique():
    cur_shops = sales.loc[sales['date_block_num'] == block_num, 'shop_id'].unique()
    cur_items = sales.loc[sales['date_block_num'] == block_num, 'item_id'].unique()
    grid.append(np.array(list(product(*[cur_shops, cur_items, [block_num]])),dtype='int32'))
​
# Turn the grid into a dataframe
grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)
​
# Groupby data to get shop-item-month aggregates
gb = sales.groupby(index_cols,as_index=False).agg({'item_cnt_day':{'target':'sum'}})
# Fix column names
gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values] 
# Join it to the grid
all_data = pd.merge(grid, gb, how='left', on=index_cols).fillna(0)
​
# Same as above but with shop-month aggregates
gb = sales.groupby(['shop_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_shop':'sum'}})
gb.columns = [col[0] if col[-1]=='' else col[-1] for col in gb.columns.values]
all_data = pd.merge(all_data, gb, how='left', on=['shop_id', 'date_block_num']).fillna(0)
​
# Same as above but with item-month aggregates
gb = sales.groupby(['item_id', 'date_block_num'],as_index=False).agg({'item_cnt_day':{'target_item':'sum'}})
gb.columns = [col[0] if col[-1] == '' else col[-1] for col in gb.columns.values]
all_data = pd.merge(all_data, gb, how='left', on=['item_id', 'date_block_num']).fillna(0)
​
# Downcast dtypes from 64 to 32 bit to save memory
all_data = downcast_dtypes(all_data)
del grid, gb 
gc.collect();
In [10]:

all_data.head()
Out[10]:
shop_id item_id date_block_num  target  target_shop target_item
0   28  7738    0   4.0 7057.0  11.0
1   28  7737    0   10.0    7057.0  16.0
2   28  7770    0   6.0 7057.0  10.0
3   28  7664    0   1.0 7057.0  1.0
4   28  7814    0   2.0 7057.0  6.0
After creating a grid, we can calculate some features. We will use lags from [1, 2, 3, 4, 5, 12] months ago.

In [11]:

# List of columns that we will use to create lags
cols_to_rename = list(all_data.columns.difference(index_cols)) 
​
shift_range = [1, 2, 3, 4, 5, 12]
​
for month_shift in tqdm_notebook(shift_range):
    train_shift = all_data[index_cols + cols_to_rename].copy()
    
    train_shift['date_block_num'] = train_shift['date_block_num'] + month_shift
    
    foo = lambda x: '{}_lag_{}'.format(x, month_shift) if x in cols_to_rename else x
    train_shift = train_shift.rename(columns=foo)
​
    all_data = pd.merge(all_data, train_shift, on=index_cols, how='left').fillna(0)
​
del train_shift
​
# Don't use old data from year 2013
all_data = all_data[all_data['date_block_num'] >= 12] 
​
# List of all lagged features
fit_cols = [col for col in all_data.columns if col[-1] in [str(item) for item in shift_range]] 
# We will drop these at fitting stage
to_drop_cols = list(set(list(all_data.columns)) - (set(fit_cols)|set(index_cols))) + ['date_block_num'] 
​
# Category for each item
item_category_mapping = items[['item_id','item_category_id']].drop_duplicates()
​
all_data = pd.merge(all_data, item_category_mapping, how='left', on='item_id')
all_data = downcast_dtypes(all_data)
gc.collect();

In [16]:

.dtypes
all_data.dtypes
Out[16]:
shop_id                 int32
item_id                 int32
date_block_num          int32
target                float32
target_shop           float32
target_item           float32
target_lag_1          float32
target_item_lag_1     float32
target_shop_lag_1     float32
target_lag_2          float32
target_item_lag_2     float32
target_shop_lag_2     float32
target_lag_3          float32
target_item_lag_3     float32
target_shop_lag_3     float32
target_lag_4          float32
target_item_lag_4     float32
target_shop_lag_4     float32
target_lag_5          float32
target_item_lag_5     float32
target_shop_lag_5     float32
target_lag_12         float32
target_item_lag_12    float32
target_shop_lag_12    float32
item_category_id        int32
dtype: object
To this end, we've created a feature matrix. It is stored in all_data variable. Take a look:

In [22]:

all_data.date_block_num.value_counts().sort_index()
Out[22]:
12    9414
13    8682
14    8526
15    7668
16    8193
17    8319
18    8247
19    7971
20    6747
21    7218
22    7509
23    8478
24    7980
25    7536
26    7704
27    6438
28    6804
29    6693
30    6474
31    3618
32    4377
33    3354
Name: date_block_num, dtype: int64
In [12]:

all_data.head(5)
all_data.head(5)
Out[12]:
shop_id item_id date_block_num  target  target_shop target_item target_lag_1    target_item_lag_1   target_shop_lag_1   target_lag_2    target_item_lag_2   target_shop_lag_2   target_lag_3    target_item_lag_3   target_shop_lag_3   target_lag_4    target_item_lag_4   target_shop_lag_4   target_lag_5    target_item_lag_5   target_shop_lag_5   target_lag_12   target_item_lag_12  target_shop_lag_12  item_category_id
0   28  10994   12  1.0 6949.0  1.0 0.0 1.0 8499.0  0.0 1.0 6454.0  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 37
1   28  10992   12  3.0 6949.0  4.0 3.0 7.0 8499.0  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.0 7521.0  0.0 0.0 0.0 37
2   28  10991   12  1.0 6949.0  5.0 1.0 3.0 8499.0  0.0 0.0 0.0 0.0 1.0 5609.0  0.0 2.0 6753.0  2.0 4.0 7521.0  0.0 0.0 0.0 40
3   28  10988   12  1.0 6949.0  2.0 2.0 5.0 8499.0  4.0 5.0 6454.0  5.0 6.0 5609.0  0.0 2.0 6753.0  0.0 0.0 0.0 0.0 0.0 0.0 40
4   28  11002   12  1.0 6949.0  1.0 0.0 1.0 8499.0  0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 40
Train/test split
For a sake of the programming assignment, let's artificially split the data into train and test. We will treat last month data as the test set.

In [18]:

Save
# Save `date_block_num`, as we can't use them as features, but will need them to split the dataset into parts 
dates = all_data['date_block_num']
​
last_block = dates.max()
print('Test `date_block_num` is %d' % last_block)
Test `date_block_num` is 33
In [67]:

dates_train = dates[dates <  last_block]
dates_test  = dates[dates == last_block]
​
X_train = all_data.loc[dates <  last_block].drop(to_drop_cols, axis=1)
X_test =  all_data.loc[dates == last_block].drop(to_drop_cols, axis=1)
​
y_train = all_data.loc[dates <  last_block, 'target'].values
y_test =  all_data.loc[dates == last_block, 'target'].values
First level models
You need to implement a basic stacking scheme. We have a time component here, so we will use scheme f) from the reading material. Recall, that we always use first level models to build two datasets: test meta-features and 2-nd level train-metafetures. Let's see how we get test meta-features first.

Test meta-features
Firts, we will run linear regression on numeric columns and get predictions for the last month.

In [73]:

rain
lr = LinearRegression()
lr.fit(X_train.values, y_train)
pred_lr_train = lr.predict(X_train.values)
pred_lr = lr.predict(X_test.values)
​
print('Train R-squared for linreg is %f' % r2_score(y_train, pred_lr_train))
print('Test R-squared for linreg is %f' % r2_score(y_test, pred_lr))
Train R-squared for linreg is 0.661589
Test R-squared for linreg is 0.743180
And the we run LightGBM.

In [72]:

rain
lgb_params = {
               'feature_fraction': 0.75,
               'metric': 'rmse',
               'nthread':1, 
               'min_data_in_leaf': 2**7, 
               'bagging_fraction': 0.75, 
               'learning_rate': 0.03, 
               'objective': 'mse', 
               'bagging_seed': 2**7, 
               'num_leaves': 2**7,
               'bagging_freq':1,
               'verbose':0 
              }
​
model = lgb.train(lgb_params, lgb.Dataset(X_train, label=y_train), 100)
pred_lgb_train = model.predict(X_train)
pred_lgb = model.predict(X_test)
​
print('Train R-squared for LightGBM is %f' % r2_score(y_train, pred_lgb_train))
print('Test R-squared for LightGBM is %f' % r2_score(y_test, pred_lgb))
Train R-squared for LightGBM is 0.474182
Test R-squared for LightGBM is 0.738391
Finally, concatenate test predictions to get test meta-features.

In [76]:

X_train_concate
X_train_concate = np.c_[pred_lr_train, pred_lgb_train]
X_test_level2 = np.c_[pred_lr, pred_lgb] 
Train meta-features
Now it is your turn to write the code. You need to implement scheme f) from the reading material. Here, we will use duration T equal to month and M=15.

That is, you need to get predictions (meta-features) from linear regression and LightGBM for months 27, 28, 29, 30, 31, 32. Use the same parameters as in above models.

In [79]:

dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]
​
# That is how we get target for the 2nd level dataset
y_train_level2 = y_train[dates_train.isin([27, 28, 29, 30, 31, 32])]
​
dates_train_level2 = dates_train[dates_train.isin([27, 28, 29, 30, 31, 32])]
In [80]:

# And here we create 2nd level feeature matrix, init it with zeros first
X_train = all_data.loc[dates <  last_block]
# datas_train
X_train_level2 = np.zeros([y_train_level2.shape[0], 2])
​
# Now fill `X_train_level2` with metafeatures
row_counter = 0
​
for cur_block_num in [27, 28, 29, 30, 31, 32]:
    
    print(cur_block_num)
    
    '''
        1. Split `X_train` into parts
           Remember, that corresponding dates are stored in `dates_train` 
        2. Fit linear regression 
        3. Fit LightGBM and put predictions          
        4. Store predictions from 2. and 3. in the right place of `X_train_level2`. 
           You can use `dates_train_level2` for it
           Make sure the order of the meta-features is the same as in `X_test_level2`
    '''      
    # Split `X_train` into parts
    X_fold_train = X_train[dates_train <  cur_block_num].drop(to_drop_cols, axis=1)
    X_fold_test =  X_train[dates_train == cur_block_num].drop(to_drop_cols, axis=1)
​
    y_fold_train = X_train[dates_train <  cur_block_num]['target'].values
    y_fold_test =  X_train[dates_train == cur_block_num]['target'].values
    print("train-X", X_fold_train.shape, "train-y", y_fold_train.shape)
    print("test-X", X_fold_test.shape, "test-x", y_fold_test.shape)
    
    # Fit linear regression 
    lr = LinearRegression()
    lr.fit(X_fold_train.values, y_fold_train)
    pred_lr = lr.predict(X_fold_test.values)
    print('Test R-squared for linreg is %f' % r2_score(y_fold_test, pred_lr))
​
    # Fit LightGBM and put predictions     
    lgb_params = {
               'feature_fraction': 0.75,
               'metric': 'rmse',
               'nthread':1, 
               'min_data_in_leaf': 2**7, 
               'bagging_fraction': 0.75, 
               'learning_rate': 0.03, 
               'objective': 'mse', 
               'bagging_seed': 2**7, 
               'num_leaves': 2**7,
               'bagging_freq':1,
               'verbose':0 
              }
    model = lgb.train(lgb_params, lgb.Dataset(X_fold_train, label=y_fold_train), 100)
    pred_lgb = model.predict(X_fold_test)
    print('Test R-squared for LightGBM is %f' % r2_score(y_fold_test, pred_lgb))
    
    # Store predictions
    X_train_level2[dates_train_level2 == cur_block_num, 0] = pred_lr
    X_train_level2[dates_train_level2 == cur_block_num, 1] = pred_lgb
    print("true-r2", r2_score(y_train_level2[dates_train_level2 == cur_block_num], y_fold_test))
    
    
# Sanity check
assert np.all(np.isclose(X_train_level2.mean(axis=0), [ 1.50148988,  1.38811989]))
27
train-X (120192, 21) train-y (120192,)
test-X (6438, 21) test-x (6438,)
Test R-squared for linreg is 0.485961
Test R-squared for LightGBM is 0.270838
true-r2 1.0
28
train-X (126630, 21) train-y (126630,)
test-X (6804, 21) test-x (6804,)
Test R-squared for linreg is 0.549966
Test R-squared for LightGBM is 0.470989
true-r2 1.0
29
train-X (133434, 21) train-y (133434,)
test-X (6693, 21) test-x (6693,)
Test R-squared for linreg is 0.796330
Test R-squared for LightGBM is 0.611470
true-r2 1.0
30
train-X (140127, 21) train-y (140127,)
test-X (6474, 21) test-x (6474,)
Test R-squared for linreg is 0.799710
Test R-squared for LightGBM is 0.641631
true-r2 1.0
31
train-X (146601, 21) train-y (146601,)
test-X (3618, 21) test-x (3618,)
Test R-squared for linreg is 0.843263
Test R-squared for LightGBM is 0.689902
true-r2 1.0
32
train-X (150219, 21) train-y (150219,)
test-X (4377, 21) test-x (4377,)
Test R-squared for linreg is 0.398219
Test R-squared for LightGBM is 0.399626
true-r2 1.0
Remember, the ensembles work best, when first level models are diverse. We can qualitatively analyze the diversity by examinig scatter plot between the two metafeatures. Plot the scatter plot below.

In [81]:

# YOUR CODE GOES HERE
​
plt.xlabel("pred-lr")
plt.ylabel("pred-gbm")
plt.scatter(X_train_level2[:, 0], X_train_level2[:, 1])
Out[81]:
<matplotlib.collections.PathCollection at 0x7f4f2f413588>

Ensembling
Now, when the meta-features are created, we can ensemble our first level models.

Simple convex mix
Let's start with simple linear convex mix:

mix=α⋅linreg_prediction+(1−α)⋅lgb_prediction
mix=α⋅linreg_prediction+(1−α)⋅lgb_prediction
 
We need to find an optimal  αα . And it is very easy, as it is feasible to do grid search. Next, find the optimal  αα  out of alphas_to_try array. Remember, that you need to use train meta-features (not test) when searching for  αα .

In [82]:

alphas_to_try = np.linspace(0, 1, 1001)
r2_of_alpha = lambda alpha: r2_score(y_train_level2, X_train_level2[:, 0] * alpha +  X_train_level2[:, 1] * (1 - alpha))
r2_result = np.array(list(map(r2_of_alpha, alphas_to_try)))
​
# YOUR CODE GOES HERE
best_alpha = alphas_to_try[np.argmax(r2_result)]
r2_train_simple_mix = np.max(r2_result)
​
print('Best alpha: %f; Corresponding r2 score on train: %f' % (best_alpha, r2_train_simple_mix))
Best alpha: 0.765000; Corresponding r2 score on train: 0.627255
Now use the  αα  you've found to compute predictions for the test set

In [83]:

best_alpha = 0.765000
​
test_preds = X_test_level2[:, 0] * best_alpha +  X_test_level2[:, 1] * (1 - best_alpha)
r2_test_simple_mix = r2_score(y_test, test_preds)
​
print('Test R-squared for simple mix is %f' % r2_test_simple_mix)
Test R-squared for simple mix is 0.781144
In [86]:

Mix
from sklearn.metrics import mean_squared_error
​
best_alpha = 0.765000
simple_mix = lambda X: X[:, 0] * best_alpha + X[:, 1] * (1 - best_alpha)
​
train_preds_mix = simple_mix(X_train_concate)
r2_train_mix = r2_score(y_train, train_preds_mix)
mse_train_mix = mean_squared_error(y_train, train_preds_mix)
​
test_preds_mix = simple_mix(X_test_level2)
r2_test_mix = r2_score(y_test, test_preds_mix)
mse_test_mix = mean_squared_error(y_test, test_preds_mix)
​
print("Mix")
print('Train R-squared %f, MSE %f' % (r2_train_mix, mse_train_mix))
print('Test  R-squared %f, MSE %f' % (r2_test_mix, mse_test_mix))
Mix
Train R-squared 0.657001, MSE 21.205593
Test  R-squared 0.781144, MSE 5.281886
Stacking
Now, we will try a more advanced ensembling technique. Fit a linear regression model to the meta-features. Use the same parameters as in the model above.

In [84]:

meta_lr = LinearRegression()
meta_lr.fit(X_train_level2, y_train_level2)
pred_meta_lr = meta_lr.predict(X_test_level2)
print('Test R-squared for linreg is %f' % r2_score(y_test, pred_meta_lr))
meta_lr = LinearRegression()
meta_lr.fit(X_train_level2, y_train_level2)
pred_meta_lr = meta_lr.predict(X_test_level2)
print('Test R-squared for linreg is %f' % r2_score(y_test, pred_meta_lr))
Test R-squared for linreg is 0.771297
Compute R-squared on the train and test sets.

In [85]:

from sklearn.metrics import mean_squared_error

train_preds = meta_lr.predict(X_train_concate)
r2_train_stacking = r2_score(y_train, train_preds)
mse_train_stacking = mean_squared_error(y_train, train_preds)

test_preds = meta_lr.predict(X_test_level2)
r2_test_stacking = r2_score(y_test, test_preds)
mse_test_stacking = mean_squared_error(y_test, test_preds)

print("Stacking")
print('Train R-squared %f, MSE %f' % (r2_train_stacking, mse_train_stacking))
print('Test  R-squared %f, MSE %f' % (r2_test_stacking, mse_test_stacking))
from sklearn.metrics import mean_squared_error
​
train_preds = meta_lr.predict(X_train_concate)
r2_train_stacking = r2_score(y_train, train_preds)
mse_train_stacking = mean_squared_error(y_train, train_preds)
​
test_preds = meta_lr.predict(X_test_level2)
r2_test_stacking = r2_score(y_test, test_preds)
mse_test_stacking = mean_squared_error(y_test, test_preds)
​
print("Stacking")
print('Train R-squared %f, MSE %f' % (r2_train_stacking, mse_train_stacking))
print('Test  R-squared %f, MSE %f' % (r2_test_stacking, mse_test_stacking))
Stacking
Train R-squared 0.649654, MSE 21.659833
Test  R-squared 0.771297, MSE 5.519535
In [77]:

train_preds = meta_lr.predict(X_train_concate)
r2_train_stacking = r2_score(y_train, train_preds)
​
test_preds = meta_lr.predict(X_test_level2)
r2_test_stacking = r2_score(y_test, test_preds)
​
print('Train R-squared for stacking is %f' % r2_train_stacking)
print('Test  R-squared for stacking is %f' % r2_test_stacking)
Train R-squared for stacking is 0.649654
Test  R-squared for stacking is 0.771297
Interesting, that the score turned out to be lower than in previous method. Although the model is very simple (just 3 parameters) and, in fact, mixes predictions linearly, it looks like it managed to overfit. Examine and compare train and test scores for the two methods.

And of course this particular case does not mean simple mix is always better than stacking.

Guess?

Mix -> Brutal search for best r-score -> Better test r-score
LR -> Optimize MSE -> Better test MSE
Failed !

In [111]:

# Try ExtraTreesRegressor
​
from sklearn.ensemble import ExtraTreesRegressor
​
meta_etsr = ExtraTreesRegressor(max_depth=5, min_samples_split=50, n_estimators=50)
meta_etsr.fit(X_train_level2, y_train_level2)
pred_meta_etsr = meta_etsr.predict(X_test_level2)
print('Test R-squared for etsr is %f' % r2_score(y_test, pred_meta_etsr))
Test R-squared for etsr is 0.801619

We all done! Submit everything we need to the grader now.
In [ ]:

from grader import Grader
grader = Grader()
​
grader.submit_tag('best_alpha', best_alpha)
​
grader.submit_tag('r2_train_simple_mix', r2_train_simple_mix)
grader.submit_tag('r2_test_simple_mix',  r2_test_simple_mix)
​
grader.submit_tag('r2_train_stacking', r2_train_stacking)
grader.submit_tag('r2_test_stacking',  r2_test_stacking)
In [ ]:

STUDENT_EMAIL = # EMAIL HERE
STUDENT_TOKEN = # TOKEN HERE
grader.status()
In [ ]:

grader.submit(STUDENT_EMAIL, STUDENT_TOKEN)
